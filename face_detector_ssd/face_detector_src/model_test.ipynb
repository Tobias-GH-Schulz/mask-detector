{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T12:19:40.055034Z",
     "start_time": "2021-03-24T12:19:37.491697Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T12:19:40.072888Z",
     "start_time": "2021-03-24T12:19:40.058294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T12:19:40.516778Z",
     "start_time": "2021-03-24T12:19:40.076285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...................\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#defining prototext and caffemodel paths\n",
    "caffeModel = \"../face_detector_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "prototextPath = \"../face_detector_model/deploy.prototxt\"\n",
    "\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "model.load_state_dict(torch.load(\"../../mnv2_mask_classifier_state.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "#Load Model\n",
    "print(\"Loading model...................\")\n",
    "net = cv2.dnn.readNetFromCaffe(prototextPath,caffeModel)\n",
    "\n",
    "frame_no = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T12:19:40.525016Z",
     "start_time": "2021-03-24T12:19:40.520617Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction_transforms = transforms.Compose([transforms.Resize((300,300)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T12:19:40.536673Z",
     "start_time": "2021-03-24T12:19:40.529165Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    image_tensor = prediction_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    #input = input.to(device)\n",
    "    output = model.forward(input)\n",
    "    index = output.detach().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-24T12:19:37.507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "# initialize the video stream to get the live video frames\n",
    "print(\"[INFO] starting video stream...\")\n",
    "video = cv2.VideoCapture(0)\n",
    "time.sleep(2.0)\n",
    "face = 0\n",
    "while(video.isOpened()):\n",
    "    check, frame = video.read()\n",
    "    if frame is not None:\n",
    "        frame_no += 1\n",
    "\n",
    "        #Get the frams from the video stream and resize to 400 px\n",
    "        frame = imutils.resize(frame,width = 300, height = 300)\n",
    "\n",
    "        # extract the dimensions , Resize image into 300x300 and converting image into blobFromImage\n",
    "        (h, w) = frame.shape[:2]\n",
    "        # blobImage convert RGB (104.0, 177.0, 123.0)\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "                                    (300, 300), (104.0, 177.0, 123.0))\n",
    "        \n",
    "\n",
    "        # passing blob through the network to detect and pridiction\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        pos_dict = dict()\n",
    "        coordinates = dict()\n",
    "\n",
    "        # Focal length\n",
    "        F = 290\n",
    "        print(detections.shape[2])\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # extract the confidence and prediction\n",
    "\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # filter detections by confidence greater than the minimum confidence\n",
    "            if confidence < 0.5 :\n",
    "                continue\n",
    "\n",
    "            # Determine the (x, y)-coordinates of the bounding box for the\n",
    "            # object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # crop the face from the frame\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "\n",
    "            # predict if person wears mask or not\n",
    "\n",
    "            face_pil = Image.fromarray(face)\n",
    "            predictions = predict_image(face_pil)\n",
    "            print(predictions)\n",
    "                \n",
    "            # draw the bounding box of the face along with the associated\n",
    "            text = \"{:.2f}%\".format(confidence * 100)\n",
    "            y = endY + 20 if endY + 20 > 10 else endY - 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                            (0, 0, 255), 2)\n",
    "            cv2.putText(frame, text, (startX, y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "            coordinates[i] = (startX, startY, endX, endY)\n",
    "\n",
    "            # Mid point of bounding box\n",
    "            x_mid = round((startX+endX)/2,4)\n",
    "            y_mid = round((startY+endY)/2,4)\n",
    "\n",
    "            height = round(endY-startY,4)\n",
    "\n",
    "            # Distance from camera based on triangle similarity\n",
    "            distance = (22 * F)/height\n",
    "            print(\"Distance(cm):{dist}\\n\".format(dist=distance))\n",
    "\n",
    "            # Mid-point of bounding boxes (in cm) based on triangle similarity technique\n",
    "            x_mid_cm = (x_mid * distance) / F\n",
    "            y_mid_cm = (y_mid * distance) / F\n",
    "            pos_dict[i] = (x_mid_cm,y_mid_cm,distance)\n",
    "\n",
    "        # Distance between every object detected in a frame\n",
    "        close_objects = set()\n",
    "        for i in pos_dict.keys():\n",
    "            for j in pos_dict.keys():\n",
    "                if i < j:\n",
    "                    dist = np.sqrt(pow(pos_dict[i][0]-pos_dict[j][0],2) + pow(pos_dict[i][1]-pos_dict[j][1],2) + pow(pos_dict[i][2]-pos_dict[j][2],2))\n",
    "\n",
    "                    # Check if distance less than 1 metres or 100 centimetres\n",
    "                    if dist < 100:\n",
    "                        close_objects.add(i)\n",
    "                        close_objects.add(j)\n",
    "                    \n",
    "        for i in pos_dict.keys():\n",
    "            if i in close_objects:\n",
    "                COLOR = [0,0,255]\n",
    "            else:\n",
    "                COLOR = [0,255,0]\n",
    "            (startX, startY, endX, endY) = coordinates[i]\n",
    "\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), COLOR, 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            # Convert cms to feet\n",
    "            cv2.putText(frame, 'Dist. to cam: {i} cm'.format(i=round(pos_dict[i][2],4)), (startX, y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        cv2.imshow(\"Face\", face)\n",
    "        cv2.resizeWindow('Frame',800,800)\n",
    "        cv2.resizeWindow('Face',800,800)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
